---
title: "STA304 - Fall 2022"
author: "Eric Zhu - 1005131368"
subtitle: "Assignment 1"
date: "21/09/2022"
output:
  pdf_document: default
---

```{r, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, eval = F)
library(lme4)
library(lmtest)
library(tidyverse)
library(extraDistr)
library(cowplot)
```

# Part 1

### Goal

This survey examines the preferences across user categories (where a user is defined as someone who actively interacts with a GPU frequently, e.g., through a scientific computing library) for specialized hardware preferences in GPUs. This includes examining the preferences of GPU capabilities across users like academics/datacentre users/professionals and consumers (who include those like gamers and hobbyists).

A Graphics Processing Unit is a microprocessor that is suited for certain kinds of computations. Fundamentally, a GPU is a SIMD device (simultaneous instruction multiple data) device that enables much faster computation of matrix heavy operations, e.g., computer graphics and certain kinds of numerical optimization algorithms (gradient descent). They were originally built to accelerate rasterization performance (2D image rendering) across computer rendering with GPU cores that performed much faster than CPU cores for rendering thanks to specialization [3]. Regardless of their origins, traditional GPU cores have been fundamental to machine learning experimentation, scientific modelling, and entertainment.

In recent years, GPUs have gotten more capabilities via specialized hardware for tasks like real-time ray tracing (simulating realistic lighting) and half-precision (16 bit) matrix math [1]. Consumers have found this to be of dubious value depending on their background and use cases, leading to discussions across the industry and community about what functions GPUs should prioritize [2]. As the GPU market continues its strong growth, slated to become a 200.85 billion dollar market by 2027 [4], the question over GPU functionality preferences also grows in importance and relevance. While there may be many preferences for GPU capabilities, the preferences we wish to focus on are based on the distinct hardware capabilities that are most relevant across consumer categories, i.e., traditional GPU cores for rasterization/compute capabilities, real-time ray-tracing hardware, and hardware for half-precision matrix math. Essentially, we are aiming to examine the question: what kind of GPU user wants what kind of GPU hardware?

### Procedure

Fundamentally, our goal is to analyse the preferences of GPUs across the consumer categories, including a diverse set of users with wildly different use cases. Thus, our target population is simply the users of GPUs across these different categories, and specifically for this survey, we are looking at two aggregated categories (professionals and consumers). The frame population is dependent on the likely ways of distributing this survey. The easiest way of distributing this survey would be through online forums of GPU users. Other ways would have significant drawbacks. For example, distributing it through exit surveys after GPU purchasing would leave us a biased same from those who use GPUs but do not directly purchase or install them, e.g., academics who run experiments on GPUs. Or the other option of using customer data collected by "game optimization" software from the manufacturer (like GeForce Experience) would bias our sample given that it is generally uncommon for enterprise settings to install these apps (e.g., AWS does not install these) and would potentially lead to biases for certain GPU manufacturers. Since forums are targeted to a specific demographic, it would make the most sense for us to employ stratified random sampling as this strategy follows most naturally from targeted forums where we would distribute our survey as they naturally form self-selecting strata. This is advantageous over other sampling methods, namely cluster random sampling. This is because it is simply unfeasible to form representative clusters with online forums, i.e., forum membership is voluntary and forum topic is predetermined. In other words, there is no good way of getting lists of users from forums like Reddit where such information is private. Thus the only feasible way would be to scrape users who comment and post as those are the only two publicly available pieces of user information. Then we'd have to send those users messages about our survey (after forming clusters), which could introduce bias should we use them as our sample since users who post generally are more vocal or passionate about some subject. So cluster random sampling could introduce worrying sources of sample bias. While stratified random sampling is definitely the better option among others, the major downside is that members of certain strata may belong in many strata, e.g., it is reasonable to assume that there are overlaps between gamers and academics (perhaps some PhD student plays video games). This may introduce some sampling bias as we might over/under represent some groups; this is unavoidable even with a different sampling technique as this source of bias is inherent to the individual so this bias would still be present regardless of sampling technique.

### Showcasing the survey.

Survey link: <https://forms.office.com/r/uqb23eNtc7>

I chose the following three questions overall because the first two are representative of questions that would enter the models as covariates, while the third question would enter the model as a response variable.

**Question 1: Estimated hours per day of intensive GPU usage? Intensive usage could be an application like training Deep Learning models, running 3D graphics applications like games, and or rendering videos.**

This question is meant to gather information measuring the participant's familiarity of GPUs. The logic here is that the longer a user uses a GPU for their specified use case (e.g., a deep learning researcher interfacing with GPU based deep learning libraries), the more familiar that user would be with GPUs and their capabilities. This is similar to the logic how we could reasonably conclude that a person is familiar the functionality of their laptop should they use it a lot. An inherent drawback to this question is that it doesn't ask directly about a user's perceived familiarity with GPUs (perhaps on some scale, e.g., 1 to 10), so it's a proxy for it. But this is done because perceived familiarity can be heavily biased by their own perception, so this provides a more absolute measure of familiarity with GPUs. Another potential drawback is that since we allow for any input, this question measures the estimated minutes on a continuous scale; perhaps a discrete scale could provide us with the ability to model this variable using random effects, but this was done because there's no good cutoff for each group. In other words, a cutoff would be misinformed and could possibly lead to skewed analysis.

**Question 2: What type of GPU do you usually use?**

Options:

-   Datacentre/HPC (Nvidia A series, AMD instinct)

-   High end consumer (Nvidia xx80 (ti), Nvidia xx90(ti), AMD x800, x800xt, x900xt, etc..., e.g., Nvidia RTX 3080)

-   Midrange consumer (Nvidia xx70 (ti), Nvidia xx60(ti), AMD x700, x700xt, x600xt, etc..., e.g., Nvidia RTX 3070)

-   Entry level consumer (Nvidia xx50 and below, AMD x500 and below, e.g., Nvidia RTX 3050)

This question is meant to gather information measuring the effect on GPU capability preferences due to a user's hardware segmentation. In other words, we are looking to gather information on if using a certain kind of GPU would affect their preferences for GPU hardware capabilities because different "levels" of GPUs have different characteristics. The logic here is comparable to asking a car owner about their current car in a survey about car feature preferences. Additionally, a strong point is that the categorization of the options is based on industry standards. As in, the datacentre/HPC categorization and corresponding examples (Nvidia A series, AMD instinct) are based on how they are marketed by both companies. For the consumer options (last 3), those are based on search results/a listing from Best Buy, a major electronics retailer. This should mean that there is very little ambiguity for the survey taker in choosing their appropriate category since this categorization would be exactly the same categories GPU buyers are exposed to. The final major pro of this question is that it is implicitly ordered, i.e., the categories are ordered by price, so we could examine how different GPU price categories affect preferences too. However, the downside to this is that some users may not necessarily agree with these categories, so they may not choose to follow the examples in the parenthesis as stated in the four options.

**Question 3: How important are traditional GPU cores in GPUs? Consider 50 to be neutral and 100 to be a strong like; negative scores will indicate a strong dislike. Decimals are allowed. **

This question is meant to gather information on a user's preference for traditional GPU cores, which is the response variable for one of the models. This question is repeated three times for the three hardware categories, i.e., traditional GPU cores, ray-tracing hardware, and half-precision matrix math hardware. We allow for decimal values so that it is appropriate later to model the response with a continuous probability distribution. So this question is meant to be interpreted along side the other two, and this is a major strength over making the user choose a particular specialized hardware preference or even a ranking. Primarily, this will allow us to examine all three hardware categories independently should the same covariates affect the response variable differently for each hardware category, i.e., we can get separate effect size estimations for the same covariates but for different response variables. A drawback is that this scale is not rigorously defined, and is the most open ended question in the survey. This is because GPU users may not consider the same scale as the other users, e.g., $50$ may be neutral to some users but may indicate a slight preference to others. To remedy this I added the "and 50 being neutral" part of the question. Nonetheless, users will likely still consider the scale to be slightly different based on their own experiences. A discrete scale, e.g., categories of preferences, could have been better as it would be more rigorously defined (labeled categories), but there would still be some bias regardless since the source of bias is the individual's conception of the scale, i.e., how they conceptualize favour for some hardware capability.

\newpage

# Part 2

## Data

We are looking to simulate the data due to practical sampling constraints. We have a few main considerations: the demographics that we are look to simulate, and the effects stemming from the covariates we wish to measure. At a high level, we are looking to measure the preferences of individuals, given their use cases, for the three kinds of hardware in GPUs we are considering: GPU cores, ray-tracing hardware, and half-precision matrix math hardware. Since it is conceivable that certain variables from our survey, e.g., type of GPU user will affect the preferences overall for a user type, we will consider the data generation in a so-called hierarchical manner.

First, we need to simulate our target population, i.e., what are the proportions of our GPU user types? While the number of each GPU user type wouldn't be released as public information (think military users or sensitive corporate situations like Amazon AWS), we can inform this simulation choice using publicly available earnings data from the two major GPU makers: Nvidia and AMD. In both cases, their public earnings data show that they sell roughly equally to the consumer and professional markets in Q1 of 2022 [5][6]. Thus, we will first sample from the Bernoulli distribution with a parameter $p=0.5$, i.e., it is equally likely for us to choose a professional or a consumer GPU user. 

Second, with a user category chosen, we then need to simulate the next three survey questions that will act as covariates in our model, i.e., factors that could affect GPU hardware category preferences. Recall that the last 3 questions are essentially response variables that ask users for their preferences of GPU hardware categories. 

The second question in our survey asks the user their primary interaction with their GPU. The question accepts three options: programming libraries (like pytorch), high level graphical interfaces (like photoshop), and graphics applications (like 3D games). The options are implicitly ordered by "invovledness" of the user's primary interaction with their GPU; in other words, we can provide an ordering to the options based on how abstracted the user's primary interaction is from dealing with the GPU. Since the ordering is based on how hands on the user's primary interaction is, we have the ordering be that programming libraries are the most hands on (a label of 3), followed by graphics applications (a label of 2), then high level graphical interfaces (a label of 1). In other words, programming libraries are least abstracted and high level graphical interfaces are most abstracted away from the GPU. Since we do not have access to how people use their GPUs, we will have to come up with some sensible values for the parameter of a categorical distribution to simulate this question. Recall that a categorical distribution takes a vector of probabilities, which in our case is dependent on the GPU user type (either professional or consumer) and is of length 3. If our user is a professional, it is sensible that the 60% of professionals primarily interact with GPUs through programming libraries (ML researchers, academics, software engineers, etc...), then 30% of professionals interact with GPUs through high level interfaces (creatives who use apps like photoshop), and then 10% of professionals interact with GPUs through graphics applications (video game designers and professionals who play a lot of video games). Then for consumers, it is sensible that 80% of consumers interact with GPUs through graphics applications (primarily video games as GPUs were initally made for them), then 10% primarily use them for programming libraries (those who may use them for ML personal projects), and then 10% primarily interact with them through high level graphical interfaces (those who have them for editing photos or videos). So to recap, we will simulate this variable by sampling from a $\text{Categorical}([0.6, 0.3, 0.1])$ distribution if they are a professional and a $\text{Categorical}([0.1, 0.1, 0.8])$ distribution if they are a consumer. 

The third question asks for the type of GPU the user uses. We unfortunately do not have data from professionals about the distribution of their GPU types since in many cases they may be custom or trade secret information. However, from having used the UofT Computer Science department GPU clusters as part of projects in the past, an institution like UofT uses almost exclusively high end or datacentre class GPUs. The CS department website also provides an example list of GPUs that are part of a general GPU cluster at UofT [8]. With that in mind, we will simulate this variable given a professional GPU user using the categorical distribution where 50% of professionals use datacentre GPUs, 40% use high end consumer models, 8% use midrange consumer models, and 2% use low end consumer models. So provided a professional GPU user we sample from $\text{Categorical}([0.5, 0.4, 0.08, 0.02])$. Alternatively, provided a consumer GPU user, we can look at the Steam hardware survey to gather some insights. Steam is a major platform used for buying/selling video games, accounting for 3.1 billion dollars in sales for the first half of 2022 [9]. While these estimates are biased towards gaming, it is the best large scale estimate of consumer GPU types. Additionally, information is collected if the Steam user opts in; it is not indicative of if the Steam user is primarily a gamer. From the Steam hardware survey, we gather that essentially no consumer uses a datacentre/HPC GPU, high end consumer GPUs account for approximately 10% of consumer GPUs, and the rest is roughly evenly split between midrange and entry level consumer GPUs. So given a consumer, we will sample this variable from $\text{Categorical}([0, 0.1, 0.45, 0.45])$. 

Then the fourth question asks about the estimated hours per week of GPU usage. This variable, as mentioned previously, is a proxy for a user's familiarity with GPUs as the longer they use a GPU through GPU based tasks (like gaming) the more familiar they should be with GPUs. This is also hard to simulate since a user's GPU usage is private information. However, we can infer that it would be sensible that professional users of GPUs use them for approximately 40 hours per week. So then, if we were to generate samples for this variable, we could use a normal distribution with a mean of 40 and a standard deviation of 5 (to account for overtime or leaving early each work day). Then for consumers, we can look to the "The State of Online Gaming â€“ 2021 survey", which surveyed 4000 people in 8 countries. The survey was conducted by Limelight, a cloud service provider. On average, they found that an average gamer played 8 hours and 27 minutes of video games per week [10]. Additionally, the top 25% of gamers play more than 12 hours per week. Using 12 hours per week as the $75^{th}$ percentile (0.674 standard deviations from the mean), we can use the sample standard deviation as the estimate for the population standard deviation, which is $\approx 5.2$ hours when solving for $0.674 \cdot \hat{\sigma}_{sample} = 3.5$ [10]. So then, if we were to generate samples for this variable, we could use a normal distribution with a mean of 9 and a standard deviation of 5.2. To recap, given a professional user, we will sample from $\mathcal{N}(40,~ 5)$, and provided a consumer, we will sample from $\mathcal{N}(9, ~5.2)$. Finally, we will absolute all of the samples should there be a negative sample as negative hours is nonsensical.

Finally, we will need to weight these simulated variables to compute a conditional expectation for the three response variables. Then, we will use this mean as the mean to a normal distribution. We will also use a constant variance parameter (one $\sigma^2$ for all $x_i$'s) because we do not have data to allow us to faithfully make simulation choices for different $\sigma^2$. 

As mentioned previously, we should model the GPU user type hierarchically, meaning that we find it reasonable that the type of user affects the baseline for GPU hardware category preferences. We will set both the professional and consumer baseline preference for traditional GPU cores as 60 because they are used in almost all GPU applications [3]. Then we will set the baseline preferences for ray-tracing hardware as much higher in consumers than professionals as that hardware is really only used in video games, so we will set the baseline preference of consumers to 50 (neutral) and 10 for professionals [2]. Finally, we set the baseline preferences for half-precision matrix math hardware as both 55 for consumers and professionals because they are used in AI task acceleration, which is rapidly used more and more in both consumer applications (video game upsampling) and professional applications (deep learning training). 

We will first consider the weights for the second survey question (a user's primary interaction with their GPU). As mentioned earlier the least abstracted primary interaction will have the greatest effect, and since programming libraries can interface with any of the GPU hardware categories, we will set the weight for this option to 4 for each of the three hardware categories. Then we will set the weight for the graphics application option to 4 for all three categories since many graphics applications (games especially) can take advantage of all three. Finally, we set the effect of the high level graphical interfaces option to 4 only for traditional GPU cores and -4 otherwise because they are really only able to take advantage of traditional GPU cores. 

Then we consider the weights for the third survey question (type of GPU). Since high end GPUs are generally very capable overall (skewing the perception of users), we will weight the datacentre and high end consumer options equally at 4 for all three categories. Then for midrange consumer GPUs, we will weight the effect for traditional GPU cores at 4, while the effect for half-precision matrix math at 2 (as they can still take advantage of it for games) and -2 for ray-tracing as they generally do not perform well at ray-tracing tasks. Finally, for entry level consumer GPUs, we will weight the effect for traditional GPU cores at 4, while the effect for the other two categories at -4 because they are generally not capable enough for either tasks. 

Finally, we consider the weights for the fourth survey question (hours of usage per week). We will weight this effect for all three categories as 0.25 since each individual hour sensibly contributes very little to a user's overall perception of GPU capabilities.

Then the individual computed conditional expectations were used as the mean for a normal distribution. Here we use a constant standard deviation for all conditional expectations simply because there is no data to otherwise inform a different modelling choice. We use a standard deviation of 5 because this allows for some amount of variation on a 100 point scale, which should represent individual variation due to sources like question interpretation/interpretations of scale (what 100 point scale means to them) despite answering the same on survey questions. 

For this simulated dataset, we simulated 1000 conditional means using the seed 123, then for each conditional mean, 10 samples were generated from the parameterized normal distribution. So overall, there are 10000 samples in this simulated dataset. 

**Note:** Since the data was simulated, there was no data cleaning involved. 

```{r, include = FALSE}
set.seed(123)
# need to generate the hierarchical effects 
# choose a user using the model user ~ bernoulli(0.5)
sim.n <- 1000
# 0 denotes professional user, 1 denotes consumer user
sim.users <- rbinom(sim.n, 1, 0.5)
# weight the effects for some user belonging to some category for a conditional expectation, i.e., E[Y|x, user] = f(x, user)
simulatePrimaryInteraction <- function(users) { 
  values <- c()
  for(i in 1:sim.n){ 
    simulation <- 0
    # rank the options in order of abstract away from GPU 
    if(users[i] == 0){
      simulation <- rcat(1, c(0.6, 0.3, 0.1), c(3, 1, 2)) 
    }else{ 
      simulation <- rcat(1, c(0.1, 0.1, 0.8), c(3, 1, 2))
    }
    values[i] <- simulation
  }
  return(values)
}

simulateGPUType <- function(users){
  values <- c()
  for(i in 1:sim.n){ 
    simulation <- 0
    if(users[i] == 0){
      simulation <- rcat(1, c(0.5, 0.3, 0.08, 0.02), c(4, 3, 2, 1))
    }else{ 
      simulation <- rcat(1, c(0, 0.1, 0.45, 0.45), c(4, 3, 2, 1))
    }
    values[i] <- simulation
  }
  return(values)
}

simulateHours <- function(users){
  values <- c()
  for(i in 1:sim.n){ 
    simulation <- 0
    if(users[i] == 0){
      simulation <- rnorm(1, 40, 5)
    }else{ 
      simulation <- rnorm(1, 9, 5.2)
    }
    values[i] <- abs(simulation)
  }
  return(values)
}

simulateConditionalExpectations <- function(combinedDf){ 
  sim.tradGPUCores <- c()
  sim.rayTraced <- c()
  sim.halfPrecision <- c()
  for(i in 1:sim.n){
    userType <- combinedDf$users[i]
    primaryInteraction <- combinedDf$primaryInteraction[i]
    gpuType <- combinedDf$GPUType[i]
    weeklyHours <- combinedDf$weeklyHours[i]
    
    tradGPUCores <- 0 
    rayTraced <- 0 
    halfPrecision <- 0
    
    # if user is professional 
    if(userType == 0){ 
      # feels less strongly
      tradGPUCores <- 60 + 4*primaryInteraction
      rayTraced <- 10 + 4 * primaryInteraction
      halfPrecision <- 55 + 4 * primaryInteraction
      
      # GPU Type - starts off low, goes high
      # Professionals may interact with more GPUs 
      tradGPUCores <- tradGPUCores + 2 * gpuType
      rayTraced <- rayTraced + 0.5 * gpuType 
      halfPrecision <- halfPrecision + 2 * gpuType

      
    }else{ 
      # they spend less time
      tradGPUCores <- 60 + 4*primaryInteraction
      rayTraced <- 50 + 2 * primaryInteraction
      halfPrecision <- 55 + 2 * primaryInteraction
      
      tradGPUCores <- tradGPUCores + 1 * gpuType
      rayTraced <- rayTraced + 0.5 * gpuType 
      halfPrecision <- halfPrecision + 1 * gpuType
      
    }
    
  
    tradGPUCores <- tradGPUCores + 0.25 * weeklyHours
    rayTraced <- rayTraced + 0.25 * weeklyHours
    halfPrecision <- halfPrecision + 0.25 * weeklyHours
    
    sim.tradGPUCores[i] <- tradGPUCores
    sim.rayTraced[i] <- rayTraced
    sim.halfPrecision[i] <- halfPrecision
  }
  combinedDf <- combinedDf %>% 
    mutate(ce.tradGPUCores = sim.tradGPUCores) %>% 
    mutate(ce.rayTraced = sim.rayTraced) %>% 
    mutate(ce.halfPrecision = sim.halfPrecision)
  
  return(combinedDf)
}

simulateSurveySample <- function(combinedDf){ 
  # num draws per cond. expectation 
  distn.draws <- 10
  columnNames <- c("user", "primaryInteraction", "GPUType", "weeklyHours", "ce.tradGPUCores", "ce.rayTraced", "ce.halfPrecision", "simulations.tradGPUCores", "simulations.rayTraced", "simulations.halfPrecision")
  finishedDf <- setNames(data.frame(matrix(ncol = 10, nrow = 0)), columnNames)

  
  for(i in 1:sim.n){
    userType <- combinedDf$users[i]
    primaryInteraction <- combinedDf$primaryInteraction[i]
    gpuType <- combinedDf$GPUType[i]
    weeklyHours <- combinedDf$weeklyHours[i]
    tradGPUCores <- combinedDf$ce.tradGPUCores[i]
    rayTraced <- combinedDf$ce.rayTraced[i]
    halfPrecision <- combinedDf$ce.halfPrecision[i]
    
    simulations.tradGPUCores <- rnorm(distn.draws, tradGPUCores, 5)
    simulations.rayTraced <- rnorm(distn.draws, rayTraced, 5)
    simulations.halfPrecision <- rnorm(distn.draws, halfPrecision, 5)
    
    
    finishedDf <- finishedDf %>% 
      add_row(
        user = userType,
        primaryInteraction = primaryInteraction,
        GPUType = gpuType,
        weeklyHours = weeklyHours,
        ce.tradGPUCores = tradGPUCores,
        ce.rayTraced = rayTraced,
        ce.halfPrecision = halfPrecision,
        simulations.tradGPUCores = simulations.tradGPUCores,
        simulations.rayTraced = simulations.rayTraced, 
        simulations.halfPrecision = simulations.halfPrecision)
  }
  
  return(finishedDf)
}

# the vector of simulated primary interactions
# 1 means programming libraries
# 2 means high level graphical interfaces
# 3 means graphics applications
sim.primaryInteraction <- simulatePrimaryInteraction(sim.users)

# the vector of simulated GPU types
# 1 means datacentre/hpc
# 2 means high end consumer 
# 3 means midrange consumer
# 4 means entry level consumer 
sim.GPUType <- simulateGPUType(sim.users)

# the vector of simulated hours per day 
sim.Hours <- simulateHours(sim.users)

# add everything to a data frame
sim.combined <- data.frame(
  users = sim.users, 
  primaryInteraction = sim.primaryInteraction,
  GPUType = sim.GPUType, 
  weeklyHours = sim.Hours
)

sim.combined <- simulateConditionalExpectations(sim.combined)

# use that conditional expectation as the mean for the normal with constant variance 

# need to generate the response variable 
# so Y ~ N(f(user), sigma^2)

sim.survey <- simulateSurveySample(sim.combined)
# write the survey to CSV if necessary 
write.csv(sim.survey, ".\\data.csv")
```

```{r}
# read the survey into a csv if necessary
sim.survey <- read_csv(".\\data.csv")
```

Below is a table of the important variables from our simulated dataset:

| Variable Name               | Description                                                                                                                                                                           |
|-----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `user`                      | The binary encoded user type, i.e., professional (0) and consumer (1)                                                                                                                  |
| `GPUType`                   | One hot encoded type of GPU (ordered by price), i.e., datacentre/HPC (4), high end consumer (3), midrange consumer (2), entry level (1)                                               |
| `primaryInteraction`        | One hot encoded user's primary interaction with a GPU (ordered by use case's abstraction from GPU) \\, i.e., programming libraries (3), graphics applications (2), high level graphical  |
| `weeklyHours`               | The number of hours per week that the user uses their GPU                                                                                                                             |
| `simulations.tradGPUCores`  | The simulated preference for traditional GPU cores \\ given other simulated survey responses                                                                                             |
| `simulations.rayTraced`     | The simulated preference for ray tracing hardware \\ given other simulated survey responses                                                                                              |
| `simulations.halfPrecision` | The simulated preference for half precision matrix math \\ hardware given other survey response                                                                                          |

Below is the five number summary of the simulated traditional GPU cores preference scores: 

| Min.  | 1st Quartile | Median | Mean  | 3rd Quartile | Max   |
|-------|--------------|--------|-------|--------------|-------|
| 54.46 | 73.93 | 77.85  | 77.86 | 81.74 | 100.64 |

Then, we have also have a five number summary of the simulated ray tracing hardware preference scores: 

| Min.    | 1st Quartile | Median   | Mean     | 3rd Quartile | Max      |
|---------|--------------|----------|----------|--------------|----------|
| 8.379 | 26.481 | 39.663 | 42.828 | 59.269 | 78.321 |

Finally, we have a five number summary of the simulated half precision matrix math hardware scores: 

| Min.  | 1st Quartile | Median | Mean  | 3rd Quartile | Max   |
|-------|--------------|--------|-------|--------------|-------|
| 46.91 | 65.19 | 69.87  | 70.20 | 75.01 | 95.80 |


From the three five number summaries, we see that the distribution of the simulated traditional GPU cores preference scores had the least variation as indicated by the IQR, the simulated half precision matrix math hardware preference scores has the second most variation, and simulated ray tracing hardware preference scores had the least. Similarly, simulated traditional GPU cores preference scores had the highest measures of centre (median and mean), simulated ray tracing hardware preference scores had the second most, and simulated half precision matrix math hardware had the least. 

```{r, include=FALSE}
# Use this to calculate some summary measures. 
summary(sim.survey$simulations.tradGPUCores)
summary(sim.survey$simulations.rayTraced)
summary(sim.survey$simulations.halfPrecision)
```
```{r, echo = F}
plot.tradGPUCores <- sim.survey %>% ggplot(aes(x = weeklyHours, y = simulations.tradGPUCores)) + geom_point(aes(color = as.factor(user))) + labs(title = "Weekly Hours of GPU Usage vs Traditional GPU Cores Preference Score", x = "Weekly Hours of GPU Usage", y = "Traditional GPU Cores Preference Score") + scale_color_discrete(name = "User Type", labels = c("Professional", "Consumer")) + theme_minimal() 

plot.rayTraced <- sim.survey %>% ggplot(aes(x = weeklyHours, y = simulations.rayTraced)) + geom_point(aes(color = as.factor(user))) + labs(title = "Weekly Hours of GPU Usage vs Ray Tracing Hardware Preference Score", x = "Weekly Hours of GPU Usage", y = "Ray Tracing Hardware Preference Score")  +  scale_color_discrete(name = "User Type", labels = c("Professional", "Consumer")) + theme_minimal()


plot.halfPrecision <- sim.survey %>% ggplot(aes(x = weeklyHours, y = simulations.halfPrecision)) + geom_point(aes(color = as.factor(user))) + labs(title = "Weekly Hours of GPU Usage vs Half Precision Hardware Preference Score", x = "Weekly Hours of GPU Usage", y = "Half Precision Matrix Math Hardware Preference Score")  +  scale_color_discrete(name = "User Type", labels = c("Professional", "Consumer")) + theme_minimal()

hist.tradGPUCores <- sim.survey %>% ggplot(aes(x = simulations.tradGPUCores)) + geom_histogram(aes(color = as.factor(user)))
```
```{r}
plot.tradGPUCores
```
From the scatter plot of `Weekly Hours of GPU Usage` versus `Traditional GPU Cores Preference Score`, we see that both distributions are approximately Gaussian; however the consumer distribution is not exactly Gaussian as weekly hours of GPU usage must be greater than 0. Additionally, the Professional category of users has a generally higher preference for traditional GPU cores. It appears that the difference between the two user groups is just a shift up/down, i.e., consumers appear to have been shifted down by some constant. The spread between the distributions appear to be similar in general.  

```{r}
plot.rayTraced
```

From the scatter plot of `Weekly Hours of GPU Usage` versus `Ray Tracing Hardware Preference Score`, we see that both distributions are also approximately Gaussian; however the consumer distribution is not exactly Gaussian as weekly hours of GPU usage must be greater than 0. Additionally, the Consumer category of users has a generally higher preference for ray tracing hardware. It appears that the difference between the two user groups is just a shift up/down, i.e., professionals appear to have been shifted down by some constant. The spread between the distributions appear to be similar in general. 

```{r}
plot.halfPrecision
```
From the scatter plot of `Weekly Hours of GPU Usage` versus `Half Precision Matrix Math Hardware Preference Score`, we see that both distributions are also approximately Gaussian; however the consumer distribution is not exactly Gaussian as weekly hours of GPU usage must be greater than 0. Additionally, the Professional category of users has a generally higher preference for half precision matrix math hardware. It appears that the difference between the two user groups is just a shift up/down, i.e., consumers appear to have been shifted down by some constant. The spread between the distributions appear to be similar in general. 


**Note about simulation:** The data was simulated specifically using the R language with base R packages and the R library `extraDistr` for sampling from a categorical distribution. 

## Methods

First from our plots, we see that in some cases, there is a pretty clear visual distinction between the preferences of GPU hardware capabilities of professional versus consumers. In particular, ray-tracing hardware seems to have a very clear difference. This indicates that potentially we will need to employ a model that considers this, i.e., a mixed effects model. At a high level, a mixed effects model is just one that allows for more granularity to the model coefficients by allowing for the coefficient of each variable to be adjusted based on some other categorical variable or for the intercept to be adjusted based some other categorical variable. While other flexible models exist, a mixed effects model is much more interpretable (consider a neural network with thousands of parameters versus a mixed effects model with a few). So our modelling approach was a hierarchical one as we believe that the preference scores are affected by an overarching variable, i.e., the user type. To gather evidence of this hierarchical nature, we employed an unpaired t-test for the means of the two distributions (they are from different groups). So we made sure it is appropriate to do so (t-test with equal variances), i.e., checked for normality (if the data conforms to the bell shaped normal distribution) using QQ-plots, and we employed the F-test for equal variances.

As a note about investigating normality, we will not use normality tests. This is because the standard Shapiro-Wilk test is too sensitive on large sample sizes, e.g., over 5000 (which is the sample size limit in R and we exceed that). So even if we have small deviations from normality, we will get evidence of it even if the tests at this sample size, e.g, T-tests, are robust to such deviations. [11] There is also the Kolmogorov-Smirnov test, but that test has low power and we must estimate our distributional parameters from data. Instead QQ-plots are preferred due to easy of interpretation at large sample sizes. [11] 

Then for the F-test for equal variances, we used a two tailed F-test using the hypotheses: 

$$
\text{H}_0: \sigma^2_\text{Professional GPU User} = \sigma^2_\text{Consumer GPU User}\\
\text{H}_a: \sigma^2_\text{Professional GPU User} \neq \sigma^2_\text{Consumer GPU User}
$$

Due to the small p-value then we have evidence that the variances of distributions among the two types of GPU users is not equal, which motivated us to use a T-test with unequal variances. 

Our T-test will compare the means of the distributions of the Professional GPU User scores to those of Consumer GPU User scores. We will do this three times, and since we are not conducting tests across scores (all tests are done separately with in each hardware preference score category), we are not performing multiple testing and do not need correction (e.g., Bonferroni correction). We used the following hypotheses: 

$$
\text{H}_0: \mu_\text{Professional GPU User} = \mu_\text{Consumer GPU User}\\
\text{H}_a: \mu_\text{Professional GPU User} \neq \mu_\text{Consumer GPU User}
$$

We are doing a two sided test, i.e., the alternative hypothesis is about inequality, because the results of this test motivate our desire to use a random effects model as we seem to need that extra granularity. In other words, since the means of these two user groups are different, we have evidence that slopes or intercepts would be potentially different if we fit linear models to each group. 


Our analysis for our results consists of mixed effects models that separately measure the preference for traditional GPU cores, ray-tracing, and half-precision matrix math hardware. In particular, we have three models using the same model specification but different response variables. So we have a model for each one of the preferences for traditional GPU cores, ray-tracing, and half-precision matrix math hardware. We construct these three models separately because we want to be able to examine each response variable, i.e., the preferences of the three categories of hardware separately. This can allow us to see if we get different effect size estimates for the same covariates with a different response variable. Recall that an effect size is simply the coefficient of a model for some variable, which is generally written as $\beta_{k}$ for the $k^{th}$ coefficient/variable. We'll specifically model the response variable using a mixed effects model, where the response follows a normal distribution. So precisely, we are using a linear mixed effects model. We make this modelling choice as the data is continuous and does not appear to be constrained (no need for a distribution like a truncated normal), and from the graphs above, they appear to be approximately normally distributed. Additionally, we add a so-called random effect due to the user type, which, as discussed above, is motivated from the observation that scores are separated by user types. In particular, we add a random intercept, meaning the intercept of the model is adjusted for each user type. We also add random slopes (adjust the coefficients for each variable by the user type) for the `Primary GPU Interaction` and `Type of GPU` variables because as discussed in the survey creation, we believe that different user types would consider the same GPU interaction and GPU type differently, meaning we'd need to adjust the coefficients based on user type. For our primary model we do not add a random slope to the `Estimated Hours of Daily GPU Usage` variable because visually it appears that coefficient for that variable is constant across user types. However we investigate this further with a more complex model and present the results in the results section. We define our primary model as follows: 

$$
\begin{aligned}
y_i &\sim N(\mu_i, \sigma^2) \\
\mu_i = \beta_0 + (\beta_1 + U_{1j})\cdot \text{Primary GPU Interaction} + (\beta_2 + U_{2j})&\cdot\text{Type of GPU}_i + \beta_3 \cdot \text{Estimated Hours of Daily GPU Usage}_i + U_{3j} \\
U_{1} &\sim N(0, \tau_1) \\
U_2 &\sim N(0, \tau_2) \\
U_{3} &\sim N(0, \tau_{3})
\end{aligned}
$$ 

Note that above, $\beta_0$ is the intercept, and $U_1j$ is the random slope for the `Primary GPU Interaction` for the $j^{th}$ user group, $U_2j$ is the random slope for the `Type of GPU` for the $j^{th}$ user group, and $U_{3j}$ is the random intercept for the $j^{th}$ user group. Then $U_1 \sim N(0, \tau_1)$, $U_2 \sim N(0, \tau_2)$, and $U_3 \sim N(0, \tau_3)$, meaning that these random effects are described by a normal distribution centered around 0 with some standard deviations. The random effects are from distributions from centered around 0 because they may not have an effect for some group, and are given independent standard deviations as this allows for more granularity, e.g., the user type may affect the `Primary GPU Interaction` differently than how it affects `Type of GPU`. 

As mentioned above we also investigate if the `Estimated Hours of Daily GPU Usage` variable also needs a random slope for the user type, i.e, if the user type affects `Estimated Hours of Daily GPU Usage` coefficient differently by user type. In this case, we use the same variable but with one additional random slope: 

$$
\begin{aligned}
y_i &\sim N(\mu_i, \sigma^2) \\
\mu_i = \beta_0 + (\beta_1 + U_{1j})\cdot \text{Primary GPU Interaction} + (\beta_2 + U_{2j})&\cdot\text{Type of GPU}_i + (\beta_3 + U_{4j}) \cdot \text{Estimated Hours of Daily GPU Usage}_i + U_{3j} \\
U_{1} &\sim N(0, \tau_1) \\
U_2 &\sim N(0, \tau_2) \\
U_{3} &\sim N(0, \tau_{3}) \\
U_{4} &\sim N(0, \tau_{4})
\end{aligned}
$$ 

For brevity sake, we will not discuss the model in detail again. The singular addition to this model is $U_{4j}$, which is the random slope that enables a different coefficient for `Estimated Hours of Daily GPU Usage` based on user type. $U_4$ is distributed by a $N(0, \tau_4)$ distribution for the same reasons above. 

Recall that we chose a linear mixed effects model over other flexible choices like neural networks because linear mixed effects models are relatively easy to interpret. However, adding this random slope makes the model more complicated to interpret, so we will use a likelihood-ratio test to determine if it is worth adding this extra complexity. To do so, we will use the R package `lmtest`.

The likelihood-ratio test examines if the more complex model has as higher likelihood under our sample, i.e., if the model is essentially a better fit. If the more complex model is significantly a better fit (has a higher likelihood), then we would choose the more complex model. So the associated hypotheses are: 

$$
H_{0}: \text{The simpler model is better} \\
H_{a}: \text{The more complex model is better}
$$

So if we have a small p-value from the resulting test, then we have evidence that the more complex model is better. We will opt to not show the derivations of the test statistic as it is rather complex and out of the scope of this paper. For clarity the test statistic we use is: 

$$
-2 \log(\frac{\mathcal{L}_{\text{simple model}}(\hat{\theta}\mid x)}{\mathcal{L}_{\text{complex model}}(\hat{\theta} \mid x)})
$$

Note that $\mathcal(\hat{\theta} \mid x)$ is the log-likelihood function provided data $x$ and maximum likelihood estimated parameters $\hat{\theta}$. 

Finally note that all of these models are fit using the R package `lmer`, and model specification as code is available in the results section. 

Once we have chosen a final model (either that with or without a random slope for the `Estimated Hours of Daily GPU Usage` variable), we will compute a confidence interval for the effects of the three models (recall we fit a model for each of the three preference scores). This will allow us to examine a range for which we are confident that our true parameters lie. Specifically R will be employed to calculate a standard Wald Confidence Interval; since this derivation is involved and out of the scope of this applied paper, we will not present the derivation. The usual Wald Confidence Interval test statistic is (for a 95% confidence interval): 

$$
\hat{\theta} \pm 1.96\cdot \text{SE}(\hat{\theta})
$$

## Results

```{r}
set.seed(123)

pro.rayTraced <- sim.survey %>% filter(user == 0) %>% select(simulations.rayTraced)
con.rayTraced <- sim.survey %>% filter(user == 1) %>% select(simulations.rayTraced)

pro.tradGPUCores <- sim.survey %>% filter(user == 0) %>% select(simulations.tradGPUCores)
con.tradGPUCores <- sim.survey %>% filter(user == 1) %>% select(simulations.tradGPUCores)

pro.halfPrecision <- sim.survey %>% filter(user == 0) %>% select(simulations.halfPrecision)
con.halfPrecision <- sim.survey %>% filter(user == 1) %>% select(simulations.halfPrecision)

qq_plot.pro.rayTraced <- pro.rayTraced %>% ggplot(aes(sample = simulations.rayTraced)) + stat_qq() + stat_qq_line() + theme_minimal()
qq_plot.con.rayTraced <- con.rayTraced %>% ggplot(aes(sample = simulations.rayTraced)) + stat_qq() + stat_qq_line() + theme_minimal()

qq_plot.pro.tradGPUCores <- pro.tradGPUCores %>% ggplot(aes(sample = simulations.tradGPUCores)) + stat_qq() + stat_qq_line() + theme_minimal()
qq_plot.con.tradGPUCores <- con.tradGPUCores %>% ggplot(aes(sample = simulations.tradGPUCores)) + stat_qq() + stat_qq_line() + theme_minimal()

qq_plot.pro.halfPrecision <- pro.halfPrecision %>% ggplot(aes(sample = simulations.halfPrecision)) + stat_qq() + stat_qq_line() + theme_minimal()
qq_plot.con.halfPrecision <- con.halfPrecision %>% ggplot(aes(sample = simulations.halfPrecision)) + stat_qq() + stat_qq_line() + theme_minimal()

```


```{r, include = FALSE}

set.seed(123)

model.tradGPUCores <- lmer(simulations.tradGPUCores ~ weeklyHours + primaryInteraction + GPUType + (1 + primaryInteraction + GPUType | user), data = sim.survey)
ranef(model.tradGPUCores)
summary(model.tradGPUCores)

model.rayTraced <- lmer(simulations.rayTraced ~ weeklyHours + primaryInteraction + GPUType + (1 + primaryInteraction + GPUType | user), data = sim.survey)

model.halfPrecision <- lmer(simulations.halfPrecision ~ weeklyHours + primaryInteraction + GPUType + (1 + primaryInteraction + GPUType | user), data = sim.survey)


model.extra.tradGPUCores <- lmer(simulations.tradGPUCores ~ weeklyHours + primaryInteraction + GPUType + (1 + primaryInteraction + GPUType + weeklyHours | user), data = sim.survey)

model.extra.rayTraced <- lmer(simulations.rayTraced ~ weeklyHours + primaryInteraction + GPUType + (1 + primaryInteraction + GPUType + weeklyHours | user), data = sim.survey)

model.extra.halfPrecision <- lmer(simulations.halfPrecision ~ weeklyHours + primaryInteraction + GPUType + (1 + primaryInteraction + GPUType + weeklyHours | user), data = sim.survey)
```
```{r}

set.seed(123)

lmtest::lrtest(model.extra.tradGPUCores, model.extra.tradGPUCores)
lmtest::lrtest(model.extra.halfPrecision, model.extra.halfPrecision)
lmtest::lrtest(model.extra.rayTraced, model.rayTraced)
```

```{r}
confint(model.rayTraced, method="Wald")
```


First, we will examine the QQ-plots of all three preference score categories, where each plot contains the QQ-plot for both consumer and professional groups. This as discussed in the methods section will examine if our data has worrying deviations from normality. 


```{r}
title.qqplot.tradGPUScores <- ggdraw() + 
  draw_label(
    "QQ-Plots of Traditional GPU Cores Preference Scores",
    fontface = 'bold',
    x = 0,
    hjust = 0
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 7)
  )
plot.row.tradGPUCores <- plot_grid(qq_plot.con.tradGPUCores, qq_plot.pro.tradGPUCores, labels = c("Consumer", "Professional"))
plot_grid(
  title.qqplot.tradGPUScores, plot.row.tradGPUCores,
  ncol = 1,
  rel_heights = c(0.1, 1)
)
```

The above plot depicts the QQ-plots for Traditional GPU Cores preference scores. In the consumer subplot, we see that the data mostly conforms to a normal distribution, where there are some deviations in the tails. Additionally, it appears in the professional subplot, we see that the data also mostly conforms to a normal distribution, but with some more deviation in the upper tail. However, overall, these are small deviations so we shouldn't be too worried about violating normality assumptions.  


```{r}
title.qqplot.tradGPUScores <- ggdraw() + 
  draw_label(
    "QQ-Plots of Ray Tracing Hardware Preference Scores",
    fontface = 'bold',
    x = 0,
    hjust = 0
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 7)
  )
plot.row.rayTraced <- plot_grid(qq_plot.con.rayTraced, qq_plot.pro.rayTraced, labels = c("Consumer", "Professional"))
plot_grid(
  title.qqplot.tradGPUScores, plot.row.rayTraced,
  ncol = 1,
  rel_heights = c(0.1, 1)
)
```

The plot above depicts the QQ-plots for ray tracing hardware preference scores by user category. In the consumer subplot, we see that the only deviations from the QQ-line is in the lower tail, where the deviation is potentially indicative of a lighter than theoretical lower tail. In the professional subplot, we see that there are essentially no deviations from the QQ-line, i.e., we are not concerned about violating normality assumptions.Overall, we should not be very concerned about violating normality assumptions since any deviations from QQ-line are minor. 


```{r}

title.qqplot.tradGPUScores <- ggdraw() + 
  draw_label(
    "QQ-Plots of Half Precision Hardware Preference Scores",
    fontface = 'bold',
    x = 0,
    hjust = 0
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 7)
  )
plot.row.rayTraced <- plot_grid(qq_plot.con.halfPrecision, qq_plot.pro.halfPrecision, labels = c("Consumer", "Professional"))
plot_grid(
  title.qqplot.tradGPUScores, plot.row.rayTraced,
  ncol = 1,
  rel_heights = c(0.1, 1)
)
```

The plot above depicts the QQ-plots for half precision matrix math hardware preference scores by user category. In the consumer subplot, we see that the only deviations from the QQ-line is in the upper tail, where the deviation is indicative of a slightly heavier than theoretical upper tail. In the professional subplot, we see a slight deviation from the QQ-line in the lower tail, where the small deviation is indicative of a slightly lighter than theoretical lower tail. Overall, we should not be very concerned about violating normality assumptions since any deviations from QQ-line are minor. 

Since we feel comfortable with our normality assumption, i.e., our distributions are all approximately normal, we conduct a T-test for the differences in means between the three score categories separately. 

```{r}
t.test(pro.tradGPUCores, con.tradGPUCores)
```

From our T-test examining the means of the traditional GPU cores preference scores of professionals and consumers, we find very strong evidence of evidence that the difference of means is not equal to 0. While the difference is small, i.e., $\approx 3$, the p-value is less than $2.2 \cdot 10^{-16}$, i.e., very strong evidence of this difference in means. Thus, we conclude that it could be appropriate to use a mixed effects model for modelling the preference scores of traditional GPU cores. 

```{r}
t.test(pro.rayTraced, con.rayTraced)
```

Then from our T-test examining the means of the ray tracing hardware preference scores of professionals and consumers, we find very strong evidence of evidence that the difference of means is not equal to 0. The difference is quite large, i.e., $\approx 33$, and the p-value is less than $2.2 \cdot 10^{-16}$, so we have very strong evidence of this difference in means. Thus, we conclude that it could also be appropriate to use a mixed effects model for modelling the preference scores of ray tracing hardware. 



```{r}
t.test(pro.halfPrecision, con.halfPrecision)
```

Finally from our T-test examining the means of the half precision matrix math hardware preference scores of professionals and consumers, we find very strong evidence of evidence that the difference of means is not equal to 0. The difference is also comparably large (compared to that of traditional GPU cores), i.e., $\approx 10$, and the p-value is less than $2.2 \cdot 10^{-16}$, so we have very strong evidence of this difference in means. Thus, we conclude that it could also be appropriate to use a mixed effects model for modelling the preference scores of half precision matrix math hardware. 


## Bibliography

1. Evanson, Nick. "Explainer: What Are Tensor Cores?" TechSpot, TechSpot, 27 July 2020, https://www.techspot.com/article/2049-what-are-tensor-cores.
2. Ravenscraft, Eric. "Should Anyone Actually Care about Ray Tracing?" Wired, Conde Nast, 3 Mar. 2021, https://www.wired.com/story/should-anyone-actually-care-about-ray-tracing.
3. Caulfield, Brian. "CPU vs GPU: What's the Difference?" NVIDIA Blog, 23 June 2022, https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu.
4. â€œGPU Processing Unit (GPU) Marketâ€ Allied Market Research, https://bit.ly/3BLAyWQ. 
5. â€œAMD Reports First Quarter 2022 Financial Results.â€ Advanced Micro Devices, Inc., 3 May 2022, https://ir.amd.com/news-events/press-releases/detail/1062/amd-reports-first-quarter-2022-financial-results. 
6. â€œNvidia Announces Financial Results for First Quarter Fiscal 2023.â€ NVIDIA Newsroom, 20 Sept. 2022, https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-first-quarter-fiscal-2023. 
7. Pandey, Mohit, et al. â€œThe Transformational Role of GPU Computing and Deep Learning in Drug Discovery.â€ Nature News, Nature Publishing Group, 23 Mar. 2022, https://www.nature.com/articles/s42256-022-00463-x. 
8. CSLab Support, https://support.cs.toronto.edu/systems/linuxgpu.html. 
9. â€œGames Industry Data and Analysis.â€ Video Game Insights, https://vginsights.com/insights/article/report-steam-games-market-size-likely-to-decline-in-2022-after-reaching-6-6bn-in-2021.
10. Combs, Veronica, et al. â€œ8 Hours and 27 Minutes. That's How Long the Average Gamer Plays Each Week.â€ TechRepublic, 22 Sept. 2022, https://www.techrepublic.com/article/8-hours-and-27-minutes-thats-how-long-the-average-gamer-plays-each-week/. 
11. Ghasemi, Asghar, and Saleh Zahediasl. â€œNormality tests for statistical analysis: a guide for non-statisticians.â€ International journal of endocrinology and metabolism vol. 10,2 (2012): 486-9. doi:10.5812/ijem.3505


\newpage

## Appendix

Here is a glimpse of the data set simulated/surveyed:

```{r, echo = FALSE}
glimpse(sim.survey)
```
