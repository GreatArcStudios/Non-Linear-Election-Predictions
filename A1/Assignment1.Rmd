---
title: "STA304 - Fall 2022"
author: "Eric Zhu - 1005131368"
subtitle: "Assignment 1"
date: "21/09/2022"
output:
  pdf_document: default
---

```{r, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(openintro)
```


# Part 1

### Goal
This survey examines the preferences across consumer categories (where consumer is defined as someone who would buy a GPU) for what they'd like to see in a GPU. This includes examining the preferences of GPU capabilities across users like academics/datacentre users/professionals and consumers (who include those like gamers and hobbyists).

 
A Graphics Processing Unit is a microprocessor that is suited for certain kinds of computations. Fundamentally, a GPU is a SIMD device (simultaneous instruction multiple data) device that enables much faster computation of matrix heavy operations, e.g., computer graphics and certain kinds of numerical optimization algorithms (gradient descent). They were originally built to accelerate rasterization performance (2D image rendering) across computer rendering with GPU cores that performed much faster than CPU cores for rendering thanks to specialization (Caulfield). Regardless of their origins, traditional GPU cores have been fundamental to machine learning experimentation, scientific modelling, and entertainment. In recent years, GPUs have gotten more capabilities via specialized hardware for tasks like real-time ray tracing (simulating super realistic lighting) and half-precision (16 bit) matrix math (Evanson). Consumers have found this to be of dubious value depending on their background and use cases, leading to discussions across the industry and community about what functions GPUs should prioritize (Ravenscraft). As the GPU market continues its strong growth, carrying over from COVID market growth, the question over GPU functionality preferences also grows in importance and relevance. While there may be many preferences for GPU capabilities, the preferences we wish to focus on are based on the distinct hardware capabilities that are most relevant across consumer categories, i.e., GPU cores for rasterization/traditional compute capabilities, real-time ray-tracing hardware, and hardware for half-precision matrix math. Essentially, we are aiming to examine the question: what kind of GPU user wants what kind of GPU hardware? 


### Procedure

Fundamentally, our goal is to analyse the preferences of GPUs across the consumer categories, including a diverse set of users with wildly different use cases. Thus, our target population is simply the users of GPUs across these different categories, and specifically for this survey, we are looking at academics, gamers, crypto currency miners, and hobbyists, who are some of the major demographic groups of GPU users. The frame population is dependent on the likely ways of distributing this survey. The easiest way of distributing this survey would be through online forums of GPU users. Other ways would have significant drawbacks. For example, distributing it through exit surveys after GPU purchasing would leave us a biased same from those who use GPUs but do not directly purchase or install them, e.g., academics who run experiments on GPUs. Or the other option of using customer data collected by "game optimization" software from the manufacturer (like GeForce Experience) would bias our sample given that it is generally uncommon for enterprise settings to install these apps (e.g., AWS does not install these) and would potentially lead to biases for certain GPU manufacturers. Since forums are targeted to a specific demographic, it would make the most sense for us to employ stratified random sampling as this strategy follows most naturally from targeted forums where we would distribute our survey as they naturally form self-selecting strata. This is advantageous over other sampling methods, namely cluster random sampling. This is because it is simply unfeasible to form representative clusters with online forums, i.e., forum membership is voluntary and forum topic is predetermined. In other words, there is no good way of getting lists of users from forums like Reddit where such information is private. Thus the only feasible way would be to scrape users who comment and post as those are the only two publicly available pieces of user information. Then we'd have to send those users messages about our survey (after forming clusters), which could introduce bias should we use them as our sample since users who post generally are more vocal or passionate about some subject. So cluster random sampling could introduce worrying sources of sample bias. While stratified random sampling is definitely the better option among others, the major downside is that members of certain strata may belong in many strata, e.g., it is reasonable to assume that there are overlaps between gamers and academics (perhaps some PhD student plays video games). This may introduce some sampling bias as we might over/under represent some groups; this is unavoidable even with a different sampling technique as this source of bias is inherent to the individual so this bias would still be present regardless of sampling technique. 

### Showcasing the survey.

Survey link: [https://forms.office.com/r/uqb23eNtc7](https://forms.office.com/r/uqb23eNtc7)

**Question 1: Estimated minutes per day of intensive GPU usage? Intensive usage could be an application like training Deep Learning models, running 3D graphics applications like games, and or rendering videos.** 

This question is meant to enter the models as a covariate measuring the familiarity of GPUs. The logic here is that the longer a user uses a GPU for their specified use case (e.g., a deep learning researcher interfacing with GPU based deep learning libraries), the more familiar that user would be with GPUs and their capabilities. This is similar to the logic how we could reasonably conclude that a person is familiar the functionality of their laptop should they use it a lot. An inherent drawback to this question is that it doesn't ask directly about a user's perceived familiarity with GPUs (perhaps on some scale, e.g., 1 to 10), so it's a proxy for it. But this is done because perceived familiarity can be heavily biased by their own perception, so this provides a more objective measure of familiarity with GPUs. Another potential drawback is that since we allow for any input, this question measures the estimated minutes on a continuous scale; perhaps a discrete scale could provide us with the ability to model this variable using random effects, but this was done because there's no good cutoff for each group. In other words, a cutoff would be misinformed and could possibly lead to skewed analysis.

**Question 2: What type of GPU do you usually use?**

Options: 
- Datacentre/HPC (Nvidia A series, AMD instinct)
- High end consumer (Nvidia xx80 (ti), Nvidia xx90(ti), AMD x800, x800xt, x900xt, etc..., e.g., Nvidia RTX 3080) 
- Midrange consumer (Nvidia xx70 (ti), Nvidia xx60(ti), AMD x700, x700xt, x600xt, etc..., e.g., Nvidia RTX 3070)
- Entry level consumer (Nvidia xx50 and below, AMD x500 and below,  e.g., Nvidia RTX 3050)


This question is meant to enter the models as a covariate measuring the effect on GPU capability preferences due to a user's hardware segmentation. 


**Question 3: On a 100 point scale (allowing for decimals),  how important is traditional GPU cores in GPUs?** 

 


\newpage

# Part 2

## Data

We are looking to simulate the data due to practical sampling constraints. We have a few main considerations: the demographics that we are look to simulate, and the effects stemming from the covariates we wish to measure. Ultimately, we are looking to measure the preferences of individuals, given their use cases, for three kinds of hardware in GPUs: GPU cores, ray-tracing hardware, and half-precision matrix math hardware. Since it is conceivable that certain variables from our survey, e.g., type of GPU user will affect the preferences on a group level, we will consider the data generation in a so-called hierarchical manner. For example, 

```{r, include = FALSE}

# Here you can load in (or simulate) and clean the data (you may need to do the cleaning in a separate R script - this is up to you). 

# You may need additional chunks, in case you want to include some of the cleaning output.

```

<Type here a summary of the cleaning process.>

<Include a description of the important variables.> 

```{r, include=FALSE}

# Use this to calculate some summary measures. 

```


<Include a description of the numerical summaries. Remember you can use `r ` to use inline R code.>




```{r, echo = TRUE}

# Use this to create some plots. 

```

<Include a clear description of the plot(s). I would recommend one paragraph for each plot.> 

All analysis for this report was programmed using `R version 4.0.2`. 


## Methods

Our analysis will consist of mixed effects models that separately measure the preference for rasterization, ray-tracing, and half-precision matrix math hardware. In particular, we will have three models using the same model specification, but just measuring different response variables. So we will have a model for each one of the preferences for rasterization, ray-tracing, and half-precision matrix math hardware. We do this since each of these variables is a real valued variable on the interval $[0, 100]$. We will specifically model the response variable using a generalized linear mixed effects model, where the response is given as a truncated normal distribution. This is because the response variable is constrained to the interval $[0,100]$. 

$$ include.your.mathematical.model.here.if.you.have.some.math.to.show $$

<Here you should describe the CI. Here is an example with a citation:>

I will invoke a non-parametric bootstrap [2] to derive the 95\% confidence interval (CI) for the mean age of students in STA304.





## Results 


```{r, include = FALSE}
library(GLMMadaptive)
```
```{r, include = F}

```

<Here you could present your results. You may want to put them into a well formatted table. Be sure that there is some text describing the results.>


<Note: Alternatively you can use the `knitr::kable` function to create a well formatted table from your code. See here: [https://rmarkdown.rstudio.com/lesson-7.html](https://rmarkdown.rstudio.com/lesson-7.html). If you do this, be sure to include this in the bibliography [3].>



## Bibliography

1. Evanson, Nick. “Explainer: What Are Tensor Cores?” TechSpot, TechSpot, 27 July 2020, https://www.techspot.com/article/2049-what-are-tensor-cores/. 
2. Ravenscraft, Eric. “Should Anyone Actually Care about Ray Tracing?” Wired, Conde Nast, 3 Mar. 2021, https://www.wired.com/story/should-anyone-actually-care-about-ray-tracing/. 
3. Caulfield, Brian. “CPU vs GPU: What's the Difference?” NVIDIA Blog, 23 June 2022, https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/. 


\newpage

## Appendix


Here is a glimpse of the data set simulated/surveyed:

```{r, echo = FALSE}

# glimpse(my_data)

```




